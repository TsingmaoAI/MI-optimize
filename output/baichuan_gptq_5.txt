INFO:root:--model: /models/Baichuan2-7B-Base
INFO:root:--algo: gptq
INFO:root:--wbit: 2
INFO:root:--device: cuda
INFO:root:Current Time: 2024-08-23 15:25:39.406649
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BaichuanTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/psh/miniconda3/envs/psh_python/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [01:07<01:07, 67.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 47.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 50.50s/it]
INFO:root:get_c4_validation
Token indices sequence length is longer than the specified maximum sequence length for this model (557878 > 4096). Running this sequence through the model will result in indexing errors
Traceback (most recent call last):
  File "/home/psh/MI-optimize/examples/baichuan/quantization.py", line 62, in <module>
    model = baichuan_sequential(model=model, data=calibrate, **args_dict)
  File "/home/psh/MI-optimize/mi_optimize/quantization/models/baichuan_seq.py", line 44, in baichuan_sequential
    model(batch.to(device))
  File "/home/psh/miniconda3/envs/psh_python/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/psh/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Base/modeling_baichuan.py", line 684, in forward
    outputs = self.model(
  File "/home/psh/miniconda3/envs/psh_python/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/psh/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Base/modeling_baichuan.py", line 420, in forward
    attention_mask = self._prepare_decoder_attention_mask(
  File "/home/psh/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Base/modeling_baichuan.py", line 358, in _prepare_decoder_attention_mask
    expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(
  File "/home/psh/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Base/modeling_baichuan.py", line 86, in _expand_mask
    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.74 GiB total capacity; 31.00 GiB already allocated; 14.12 MiB free; 31.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
INFO:root:--model: /models/Baichuan2-7B-Base
INFO:root:--algo: gptq
INFO:root:--wbit: 3
INFO:root:--device: cuda
INFO:root:Current Time: 2024-08-23 15:28:03.736965
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'BaichuanTokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/psh/miniconda3/envs/psh_python/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
